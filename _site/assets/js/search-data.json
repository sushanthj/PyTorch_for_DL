{"0": {
    "doc": "Basics",
    "title": "Basics",
    "content": "{: .text-delta } 1. TOC {:toc} # The following document sumarizes the PyTorch concepts Read this doc after maybe 10% of the Coursera course: https://courses.cs.washington.edu/courses/cse446/19au/section9.html ",
    "url": "/Basics/",
    "relUrl": "/Basics/"
  },"1": {
    "doc": "Derivatives in PyTorch",
    "title": "Derivatives in PyTorch",
    "content": "{: .text-delta } 1. TOC {:toc} # Derviates ![](/images/derivates.jpeg) ## Understanding the variables in derivatives If we have a simple y=x**2 function, the way each tensor gets defined changes. Notice how we define the tensor x: ```python # Create a tensor x x = torch.tensor(2.0, requires_grad = True) print(\"The tensor x: \", x) ``` Now notice how we define y: ```python # Create a tensor y according to y = x^2 y = x ** 2 print(\"The result of y = x^2: \", y) ``` Now that the tensors are defined, there are two steps we need to follow to get the derivative at a specific point: 1. y.backward() 2. x.grad On their own, each would not be sufficient to find grad. However, y.backward() does the actual math part of derivation in the background and the *gradient is stored in the x tensor* Notice the following attributes of x and y and their respective outputs: ![](/images/derivates.jpeg) ## Passing multiple values as input to a function whose gradient we find Here we use the similar linspace function (like numpy) to get a range of values: ```python # Calculate the derivative with multiple values x = torch.linspace(-10, 10, 10, requires_grad = True) Y = x ** 2 y = torch.sum(x ** 2) ``` ## Plotting the above function in matplotlib ```python # Take the derivative with respect to multiple value. Plot out the function and its derivative y.backward() plt.plot(x.detach().numpy(), Y.detach().numpy(), label = 'function') plt.plot(x.detach().numpy(), x.grad.detach().numpy(), label = 'derivative') plt.xlabel('x') plt.legend() plt.show() ``` In the example, we use x.detach and y.detach \\ What this does is ensure that further grads or any values are not added as attributes to x and y tensors Note: The tensors use graphs to populate values of x.grad and y.grad. Hence, these detach functions \\ essentially just disable any further sub-graphs being populated to x or y # Partial Derivaties ![](/images/partial_derivatives.jpeg) # Differentiation in Forward/Backward pass of a Neural Net Previously we used 'y.backward' when doing the differentiation Now, let's customize the 'backward' function according the math operator Let's say: **y = 2x** ```python class SQ(torch.autograd.Function): @staticmethod def forward(ctx,i): \"\"\" In the forward pass we receive a Tensor containing the input and return a Tensor containing the output. ctx is a context object that can be used to stash information for backward computation. You can cache arbitrary objects for use in the backward pass using the ctx.save_for_backward method. \"\"\" result=i**2 ctx.save_for_backward(i) return result @staticmethod def backward(ctx, grad_output): \"\"\" In the backward pass we receive a Tensor containing the gradient of the loss with respect to the output, and we need to compute the gradient of the loss with respect to the input. \"\"\" i, = ctx.saved_tensors grad_output = 2*i return grad_output ``` We will now apply the class above using a .apply tag as shown below: ```python x=torch.tensor(2.0,requires_grad=True ) sq=SQ.apply y=sq(x) y print(y.grad_fn) y.backward() x.grad ``` ## Torch.is_leaf ![](/images/is_leaf.jpeg) ",
    "url": "/Derivatives/",
    "relUrl": "/Derivatives/"
  },"2": {
    "doc": "Git Concepts",
    "title": "Git Concepts",
    "content": "{: .text-delta } 1. TOC {:toc} # Before you Begin {: .fs-9 } [Reference](https://www.w3schools.com/git/git_getstarted.asp?remote=github){: .btn .fs-5 .mb-4 .mb-md-0} In the above link, follow the procedures, but instead of using username and password each time, setup the ssh keys and use them more often *ssh keys are found in ./.ssh folder (or lookup keygen to generate your keys)* # Basics of generating new content in local and pushing to github ## Process for adding to a github page git add . \\ git commit -m \"made new code\" \\ git push or git push origin develop (if you cloned from develop branch) ## If you want to track a different branch - git branch --set-upstream-to=origin/master \\ git add . \\ git push or make a new remote - git remote add ts_origin_wiki git@github.com:sjayanth21/BR_Wiki.git \\ git push --set-upstream ts_origin_wiki master \\ git push ts_origin_wiki_master ## Working with remotes Any folder can have a number of remotes like: origin and ts_origin_github To make local branch master track a different remote branch (branch in your cloud github repo) do: git branch --set-upstream-to=origin/master or git branch --set-upstream-to=origin/develop ## If you cloned a repo, forked your own branch (using git checkout) You may need to pull from upstream to update your codebase \\ However, running a simple 'git pull' may throw merge conflicts So do the following 1. Run a 'git fetch' to get the updates on all branches (and if any new branch has been added) 2. In your personal branch commit all changes by doing: git add, commit and push 3. sudo apt install meld 4. Now to get the upstream updates do 'git checkout develop' (whichever is the main branch) 5. Now to put this in your personal branch run 'git checkout feature/sj' 6. Now we do the actual merging using 'git merge develop' (this will merge everythin in deveop into the current branch viz feature/sj) 7. The above step would have thrown some merge conflicts, to solve that run 'git mergetool' 8. The above step opens meld, make all necessary resolutions and save 9. Now our codebase would have been updated to whatever we resolved in meld 10. Now run 'git commit' without any arguments as it is a 'merge commit' 11. Now as usual do 'git push origin feature/sj' to push your updated personal branch to github ## Points to Note - If you checkout a file 'git checkout blade.py' it resets the file to whatever is the latest from that branch in upstream - If you want to physically add or change remotes go to the respective folder and do 'nano .git/config' - the correct syntax for the merge command is: \\ 'git merge ts_origin/master' \\ What this does is that if the current branch is origin/develop it will merge the files of \\ current branch i.e origin/develop with ts_origin/master - Note that even if ts_origin/master is in ts_github account and origin/master is in sushanthj github account, it will still merge as long as remotes exist for both these accounts. If remotes don't exist, you can always add as shown up above ### Concepts for working with two repos or two repos on two different github accounts: Basically locally you will have 'master' branch if you do 'git branch' \\ This master can track two upstream branches using two different remotes \\ One remote is added automatically when you clone the repo \\ The next remote will have to be added manually to your other git account or other repo Then to push the same commit to both branches first do 'git push' \\ and see which repo it pushes to (say it pushes to origin/master \\ Then do 'git push --set-upstream ts_origin/develop' to push to your second repo \\ However, do note that your local branch always tracks to the latest branch you pushed to \\ i.e if you do a git pull, it will pull from the latest branch to which you pushed \\ in this case it will pull from ts_origin/develop ### Saving a patch file If you have changes made which you want to save locally and not push to remote, you can save a patch file ```bash git diff > new_changes.patch ``` Now to apply this patch onto any branch, do: ```bash git apply new_changes.patch ``` ### Saving changes by stashing Instead of saving a specific file for changes (such as a patch file), you could also stash your changes locally ``` git stash ``` The above command will stash all tracked changes. You could also stash only committed changes. Refer: [stashing](https://www.atlassian.com/git/tutorials/saving-changes/git-stash) To then apply the stashed changes (one time use only as pop will remove from stash) ``` git stash pop ``` To apply without popping do: ``` git stash apply ``` To remove any particular item in stash: ``` git stash drop ``` To view all entries in stash and then apply specific one do: ``` git stash list git stash apply n ``` n = stash item number ",
    "url": "/git_concepts",
    "relUrl": "/git_concepts"
  },"3": {
    "doc": "Intro",
    "title": "Intro",
    "content": "For Jekyll reference see [just_the_docs](https://pmarsceill.github.io/just-the-docs/) The following pages are built in order to understand Computer Vision and Machine Learning To deploy on heroku follow the steps in the link below (and use the gem files, rake files and proc files in this repo for reference) The following files will need to be copied from this repo: - config.ru - Rakefile - Procfile - static.json - config.yaml (only the differences) And only if necessary: - Gemfile - Gemfile.lock - remove _sites from .gitignore Run bundle exec jekyll serve after making the above changes After copying these files (or their necessary contents), install heroku cli and do: ```bash heroku login ``` Then do heroku create as per the below link and the other steps necessary (git push heroku master) [Deploy jekyll on heroku](https://blog.heroku.com/jekyll-on-heroku) Finally, go to heroku page -> settings -> change the name of the app and find the url ",
    "url": "/intro/",
    "relUrl": "/intro/"
  },"4": {
    "doc": "PyTorch Tensors",
    "title": "PyTorch Tensors",
    "content": "{: .text-delta } 1. TOC {:toc} # Basics In PyTorch all parts of a neural net like weights, datasets, biases, outcomes and FC layers are treated as tensors. One can say that a tensor is a very generalized term for any unit of data in a neural net which is capable of undergoing math operations (like vector ops) ## Creating Tensors All tensors are created in the form CHW (Channel, Height, Width) ```python import torch # three ways of initializing tensors a = torch.tensor([7,4,3,2,6]) b = torch.tensor([0.0, 1.0, 2.0, 3.0], dtype=torch.int32) c = torch.FloatTensor([0,1,2]) ``` ``` >> a[0] = 7 >> a.dtype = torch.int64 >> a.type = LongTensor >> a.size() = torch.tensor(1,5) # you can also try a.ndimension() ``` ## Resizing Tensors Note in the last line of code above we saw that tensor 'a' had 1 row and 5 columns Now to resize we do: ```python a_col = a.view(5,1) # the arguments of view dictate the no. of rows and columns respectively a_col_auto = a.view(-1,1) # -1 infers no. of rows after readjusting the second dim. ``` # Numpy and Tensor relations Preferably don't use numpy and work only with tensors in DL networks as it messes up with ONNX or TRT conversions We will use the function torch.from_numpy and a.numpy() ```python import numpy as np import torch a = np.array([1,2,3,4,5]) torch_tensor = torch.from_numpy(a) numpy_array = torch_tensor.numpy() ordinary_list = torch_tensor.to_list() ``` # Broadcasting Some torch functions can be broadcast along all elements in a tensor or a list ```python import torch import numpy as np a = torch.tensor([0, np.pi/2, 0]) a = torch.sin(a) >> a = [0,1,0] ``` ## Creating an evenly spaced torch tensor We can use the broadcasting feature of a tensor as we described above along with a torch function called linspace Note. linspace is also available in numpy with the function having the same name ```python import torch a = torch.linspace(-2,2,num=5) >> a = [-2, -1, 0, 1, 2] ``` # 2D and 3D Tensors and subscripting In matrices or numpy arrays, accessing individual elements is often called **subscripting** if 'a' is a 2D tensor, then the individual elements can be accesses as: ```python a = [...] >> a[0,2] = # 0th row and 2nd column element ``` # Converting Tensors from one type to another ![](/images/tensor_conversion.jpeg) ",
    "url": "/Tensors/",
    "relUrl": "/Tensors/"
  },"5": {
    "doc": "PyTorch Transforms and Functionals",
    "title": "PyTorch Transforms and Functionals",
    "content": "{: .text-delta } 1. TOC {:toc} All transforms come under the module **torchvision.transforms** and can be used within the layer of a neural net we usually import this as: ```python import torchvision.transforms as T ``` # Basic operations without transforms ## Changing Image Channels (aka flipping elements in any specified dimension) The way an image is loaded varies with implementations: - pytorch: CHW (channels, width, height) - opencv: HWC (height, width, channels) - numpy: WHC (width, height, channels) *this is also what imagej shows* FYI, **loading an image in opencv loads it as BGR channels** \\ To correct this we usually do cv2.cvt_Color(image, cv2.BGR2RGB) However, if we need to do the same in numpy or on torch, we essentially just need to flip the dimensions of channels B and R (or we just say flip dimension no. 0 of image tensor) ```python import torch flipped_image = torch.flip(input_tensor,[0]) # as CHW, C is in dimension [0] ``` # Tensor operations using torchvision.transform *Note: To understand better about image loading, lookup the 'Dataset Examples/image_dataset_from_scratch'* ## Padding There are two ways of padding: ### Padding using a mask tensor Lets say we have an image of size (CHW) = (3, 1200, 1300) \\ Now let's assume we need to convert it into a square image *Note. I've used padding on a rectangular image mostly in order to maintain aspect ratio during resize operations* #### Create a zero mask ```python import torch pad_mask = torch.as_tensor(torch.zeros(3,1300,1300, dtype=torch.int8)) ``` #### Copy the original image onto the mask Now the original image was of size: - orig image: (3,1200,1300) - pad_mask: (3,1300,1300) Therefore, we see that we need to only pad 50 pixels above and below in orig image ```python pad_mask[:,50:1250,:] = orig_image ``` pad_mask will be our final padded image now ### Padding using a torch transform ```python import torch import torchvision.transforms as T # we pass a tuple to the pad function below where (x,y) # x = pad amount in left and right # y = pad amount in top and bottom padder = T.Pad((0,50)) padded_img = padder(input_image) ``` ## Resizing ```python import torchvision.transforms as T # define the interpolation method # we're using NEAREST as default BILINEAR is not supported during ONNX conversion interpolation = T.InterpolationMode.NEAREST resizer = T.Resize((640,640), interpolation=interpolation) resized_image = resizer(input_image) ``` ## Multiple transforms at once (Composing): ```python from torchvision import transforms data_transform = transforms.Compose([transformer_1(), transformer_2()]) x, y = data_transform(x1, y1) ``` # Torch Functionals Many common tensor operations (used in DNNs) and few other meta operations are bundled in a class called **Module** Let's create a custom layer by subclassing the Module class (standard procedure) ```python import torch.nn as nn class Custom_Layer(nn.Module): def __init__(self): super().__init__() self.linear = nn.Linear(2,3) def forward(self,input): x = self.linear(input) return x ``` Here we saw that **nn.Linear** was used, but there are also other functions like \\ nn.Conv2D and nn.ReLu which are commonly used These functions like nn.ReLu are said to be of the form **torch.nn.functional** i.e. Relu and Conv2D are all **torch functionals** ## Using torchvision.transformations inside functionals Let's club together resizing and padding and put them in a functional called **Sequential** As the name suggests, Sequential executes anhy operations one after another ```python import torchvision.transforms as T import torch interpolation = T.InterpolationMode.NEAREST transformer = torch.nn.Sequential(T.Pad((0,pad_const)),T.Resize((640,640),interpolation=interpolation)) output_image = transformer(input_image) ``` ## Creating Meta Models Let's say we have an existing model in pytorch called *pretrained_model* \\ and we want to add additional layers to it in the beginning or the end Usually a model has a few class variables as well like stride, no. of classes etc. We will have to retain this info in our meta model as well Now let's define our custom model: ```python import torch import torchvision.transforms as T class Custom_Layer(nn.Module): # Custom layer for preprocessing def __init__(self): super().__init__() def forward(self,input_tensor): input_tensor = input_tensor.type(torch.cuda.FloatTensor) #print(\"input tensor type: \", input_tensor.dtype) pad_mask = torch.as_tensor(torch.zeros(3, 1328, 1328, dtype=torch.float32), device=0) flipped_image = torch.flip(input_tensor,[1]) pad_mask[:,64:1264,:] = flipped_image interpolation = T.InterpolationMode.NEAREST #transformer = torch.nn.Sequential(T.Pad((0,pad_const)),T.Resize((640,640),interpolation=interpolation)) transformer = torch.nn.Sequential(T.Resize((640,640),interpolation=interpolation)) transformed_tensor = transformer(pad_mask).unsqueeze(0) return transformed_tensor class Custom_Model(nn.Module): def __init__(self, pretrained_model, nc, names, stride): super(Custom_Model, self).__init__() self.nc = nc self.names = names self.preproc_layers = Custom_Layer() self.pretrained = pretrained_model self.stride = stride def forward(self, input): # add with no grad condition for the next line? mod = self.preproc_layers(input) mod = self.pretrained(mod) return mod ``` We have defined in the above code a custom layer and a custom model. You may have observed that we used torch.cuda.float16. This is because the existing tensors of the pretrained_model are all loaded onto GPU. To interface with them, we need to initialize our new tensors on the GPU as well. Now, let's call this custom model in a seperate function: ```python def custom_load(weights_file_path, device): existing_model, stride = attempt_load(weights=weights_file_path, map_location=device, inplace=True, fuse=True) nc_existing, names_existing = existing_model.nc, existing_model.names extended_model = Custom_Model(pretrained_model=existing_model, nc=nc_existing, names=names_existing, stride=stride) return extended_model ``` **Note in any model which is constructed using the nn.Module class, you do not need to specify the forward function, you just need to call the custom model class as: *output = Custom_Model(input)*** ",
    "url": "/transforms_functionals/",
    "relUrl": "/transforms_functionals/"
  },"6": {
    "doc": "Datasets & Dataloaders",
    "title": "Datasets & Dataloaders",
    "content": "{: .text-delta } 1. TOC {:toc} # Dataset examples To create our own dataset, we need to subclass the 'Dataset' class from torch.utils.data **The torch.utils.data also has an in-built DataLoader** Main Points to note: - Each dataset class we define customly, should have the functions **__init__**, **__len__**, and **__getitem__** - Each transformer class we define must have **__init__** and **__call__** functions ## Simple Dataset Refer this link to understand basic datasets (and torchvision.transforms on this dataset): [simple_dataset](/ref_code/simple_dataset.ipynb) ## Image Dataset 1. [image_dataset_from_scratch](/ref_code/image_dset_from_scratch.ipynb) 2. [image_dataset_pre_built](/ref_code/image_dset_built.ipynb) ## Sample Datasets Torchvision has some freely avialable datasets which one can use for a cursory evaluation of a model ```python import torchvision.datasets as dsets dataset = dsets.MNIST(root='./data/', train=False, download=True, transform=transforms.ToTensor()) ``` *Note: in the above dataset, if we say train=False, then we are using the test dataset (smaller set)* *Side Note: Usually train, dev and test are split in 80,10,10 ratio from the total no. of images* ## Adding preprocessing to a model: the dataloader way The basic layout of a dataset class is shown below. As mentioned earlier it needs to have three basic functions of init, len, getitem: ```python # Define class for dataset class toy_set(Dataset): # Constructor with defult values def __init__(self, length = 100, transform = None): self.len = length self.x = 2 * torch.ones(length, 2) self.y = torch.ones(length, 1) self.transform = transform # Getter def __getitem__(self, index): sample = self.x[index], self.y[index] if self.transform: sample = self.transform(sample) return sample # Get Length def __len__(self): return self.len ``` Now, we can add some preprocessing to the dataset loader by giving a transformer as an argument to the dataset class: ```python # Combine two transforms: crop and convert to tensor. Apply the compose to toy_set croptensor_data_transform = transforms.Compose([transforms.CenterCrop(20), transforms.ToTensor()]) dataset = toy_set(transform=croptensor_data_transform ) print(\"The shape of the first element tensor: \", dataset[0][0].shape) ``` ## Load images from a folder and image_names in csv as a dataset: ```python class Dataset(Dataset): # Constructor def __init__(self, csv_file, data_dir, transform=None): # Image directory self.data_dir=data_dir # The transform is goint to be used on image self.transform = transform data_dircsv_file=os.path.join(self.data_dir,csv_file) # Load the CSV file contians image info self.data_name= pd.read_csv(data_dircsv_file) # Number of images in dataset self.len=self.data_name.shape[0] # Get the length def __len__(self): return self.len # Getter def __getitem__(self, idx): # Image file path img_name=os.path.join(self.data_dir,self.data_name.iloc[idx, 1]) # Open image file image = Image.open(img_name) # The class label for the image y = self.data_name.iloc[idx, 0] # If there is any transform method, apply it onto the image if self.transform: image = self.transform(image) return image, y ``` Creating a object of dataset class: ```python dataset = Dataset(csv_file=csv_file, data_dir=directory) ``` # Dataloader Both Dataset and Dataloader are inbuilt classes in pytorch We use both in conjunction in cases such as in Stochastic Gradient Descent where each \\ datapoint from the dataset is executed one by one (instead of a bunch of datapoints \\ such as in batch gradiet descent) See the example of SGD below where we define a **dataset and dataloader**: ```python from torch.utils.data import Dataset, DataLoader # Dataset Class class Data(Dataset): # Constructor def __init__(self): self.x = torch.arange(-3, 3, 0.1).view(-1, 1) self.y = 1 * self.x - 1 self.len = self.x.shape[0] # Getter def __getitem__(self,index): return self.x[index], self.y[index] # Return the length def __len__(self): return self.len # Create the dataset and check the length dataset = Data() print(\"The length of dataset: \", len(dataset)) # Create DataLoader trainloader = DataLoader(dataset = dataset, batch_size = 1) # Training the model w = torch.tensor(-15.0,requires_grad=True) b = torch.tensor(-10.0,requires_grad=True) LOSS_Loader = [] def train_model_DataLoader(epochs): # Loop for epoch in range(epochs): # SGD is an approximation of out true total loss/cost, in this line of code we calculate our true loss/cost and store it Yhat = forward(X) # store the loss LOSS_Loader.append(criterion(Yhat, Y).tolist()) for x, y in trainloader: # make a prediction yhat = forward(x) # calculate the loss loss = criterion(yhat, y) # Backward pass: compute gradient of the loss with respect to all the learnable parameters loss.backward() # Updata parameters slope w.data = w.data - lr * w.grad.data b.data = b.data - lr* b.grad.data # Clear gradients w.grad.data.zero_() b.grad.data.zero_() train_model_DataLoader(10) ``` ",
    "url": "/Datasets+Dataloaders/",
    "relUrl": "/Datasets+Dataloaders/"
  },"7": {
    "doc": "Home",
    "title": "Home",
    "content": "# PyTorch for Deep Learning {: .fs-9 } [Courseware](https://www.coursera.org/learn/deep-neural-networks-with-pytorch/home/welcome){: .btn .fs-5 .mb-4 .mb-md-0 } ",
    "url": "/",
    "relUrl": "/"
  },"8": {
    "doc": "PyTorch Models",
    "title": "PyTorch Models",
    "content": "{: .text-delta } 1. TOC {:toc} # Creating Simple Models using torch.nn Even the simplest linear regression problem has to be bundled in a model in PyTorch. PyTorch allows for creating simple linear regression models (which are of the form y = b + wx) in a straightforward manner called *nn.Linear* We will make a custom model which does only linear regression in the torch way: ```python import torch from torch import nn # Customize Linear Regression Class class LR(nn.Module): # Constructor def __init__(self, input_size, output_size): # Inherit from parent super(LR, self).__init__() self.linear = nn.Linear(input_size, output_size) # Prediction function def forward(self, x): out = self.linear(x) return out ``` Now we can create an object of the above calss, try to feed it single or multiple inputs ```python # Create the linear regression model. Print out the parameters. lr = LR(1, 1) print(\"The parameters: \", list(lr.parameters())) print(\"Linear model: \", lr.linear) # Try our customize linear regression model with single input x = torch.tensor([[1.0]]) yhat = lr(x) print(\"The prediction: \", yhat) # Try our customize linear regression model with multiple input x = torch.tensor([[1.0], [2.0]]) yhat = lr(x) print(\"The prediction: \", yhat) ``` **Note in any model which is constructed using the nn.Module class, you do not need to specify the forward function, you just need to call the custom model class as: *output = Custom_Model(input)*** ## Accessing model params Using the above model class as an example, we can get the model weights and biases by: ```python model = custom_model(input) print(list(model.parameters())) ``` We can also access these weights and biases and even modify them using **model.state_dict()** ```python model = custom_model(input) # init weights for the layer 'linear' model.state_dict()['linear.weight'].data[0] = torch.tensor([0.51]) model.state_dict()['linear.bias'].data[0] = torch.tensor([0.4]) print(model.state_dict()) ``` ## Creating Meta Models Let's say we have an existing model in pytorch called *pretrained_model* \\ and we want to add additional layers to it in the beginning or the end Usually a model has a few class variables as well like stride, no. of classes etc. We will have to retain this info in our meta model as well Now let's define our custom model: ```python import torch import torchvision.transforms as T class Custom_Layer(nn.Module): # Custom layer for preprocessing def __init__(self): super().__init__() def forward(self,input_tensor): input_tensor = input_tensor.type(torch.cuda.FloatTensor) #print(\"input tensor type: \", input_tensor.dtype) pad_mask = torch.as_tensor(torch.zeros(3, 1328, 1328, dtype=torch.float32), device=0) flipped_image = torch.flip(input_tensor,[1]) pad_mask[:,64:1264,:] = flipped_image interpolation = T.InterpolationMode.NEAREST #transformer = torch.nn.Sequential(T.Pad((0,pad_const)),T.Resize((640,640),interpolation=interpolation)) transformer = torch.nn.Sequential(T.Resize((640,640),interpolation=interpolation)) transformed_tensor = transformer(pad_mask).unsqueeze(0) return transformed_tensor class Custom_Model(nn.Module): def __init__(self, pretrained_model, nc, names, stride): super(Custom_Model, self).__init__() self.nc = nc self.names = names self.preproc_layers = Custom_Layer() self.pretrained = pretrained_model self.stride = stride def forward(self, input): # add with no grad condition for the next line? mod = self.preproc_layers(input) mod = self.pretrained(mod) return mod ``` We have defined in the above code a custom layer and a custom model. You may have observed that we used torch.cuda.float16. This is because the existing tensors of the pretrained_model are all loaded onto GPU. To interface with them, we need to initialize our new tensors on the GPU as well. Now, let's call this custom model in a seperate function: ```python def custom_load(weights_file_path, device): existing_model, stride = attempt_load(weights=weights_file_path, map_location=device, inplace=True, fuse=True) nc_existing, names_existing = existing_model.nc, existing_model.names extended_model = Custom_Model(pretrained_model=existing_model, nc=nc_existing, names=names_existing, stride=stride) return extended_model ``` # Model Training For each iteration we need to calculate the forward pass and backward pass in any neural network To begin with, let's start with a simple linear regression of a function y = wx (where w is the weights which will be learnt) \\ *Note. here the w also represent the slope of the line because of the way we have represented it in the equation* Let's create w as: \\ ``` w = torch.tensor(-10.0, requires_grad = True) ``` Now let's define the forward and backward passes: ```python def forward(x): return w * x # essentially calculates the loss def criterion(yhat, y): return torch.mean((yhat - y) ** 2) ``` Now that we have the backbone, let's see our training loop: ```python def train_model(iter): for epoch in range (iter): # make the prediction as we learned in the last lab Yhat = forward(X) # calculate the iteration loss = criterion(Yhat,Y) # plot the diagram for us to have a better idea gradient_plot(Yhat, w, loss.item(), epoch) # store the loss into list LOSS.append(loss.item()) # backward pass: compute gradient of the loss with respect to all the learnable parameters loss.backward() # updata parameters w.data = w.data - lr * w.grad.data # zero the gradients before running the backward pass w.grad.data.zero_() train(4) ``` In the above we have few steps which are important to note: 1. The loss.backward() calcualtes the cost (which is the mean of all losses) and in the process also makes a note of all the losses (aka gradients) 2. The w.grad.data gets the losses w.r.t the weights by differentiating the cost function w.r.t the weights 3. As we see above, the w.grad.data needs to be reinitialized to zero after each epoch (epoch is just one iteration of our train function) ## Side Note We need to understand the difference between loss and cost. \\ In official terms, the loss is the difference (yhat-y)**2, which we call the loss function Also, the cost is the mean of the all loss over all variables (say if x and y were a 1D vector of size 10) However, in pytorch the cost is called 'loss' directly ",
    "url": "/torch.nn/",
    "relUrl": "/torch.nn/"
  },"9": {
    "doc": "Plotting Loss Landscapes",
    "title": "Plotting Loss Landscapes",
    "content": "{: .text-delta } 1. TOC {:toc} We will primarily use matplotlib and numpy to do these 2D or 3D plots: # The plotting class ```python # The class for plot the diagram class plot_error_surfaces(object): # Constructor def __init__(self, w_range, b_range, X, Y, n_samples = 30, go = True): W = np.linspace(-w_range, w_range, n_samples) B = np.linspace(-b_range, b_range, n_samples) w, b = np.meshgrid(W, B) Z = np.zeros((30, 30)) count1 = 0 self.y = Y.numpy() self.x = X.numpy() for w1, b1 in zip(w, b): count2 = 0 for w2, b2 in zip(w1, b1): Z[count1, count2] = np.mean((self.y - w2 * self.x + b2) ** 2) count2 += 1 count1 += 1 self.Z = Z self.w = w self.b = b self.W = [] self.B = [] self.LOSS = [] self.n = 0 if go == True: plt.figure() plt.figure(figsize = (7.5, 5)) plt.axes(projection = '3d').plot_surface(self.w, self.b, self.Z, rstride = 1, cstride = 1,cmap = 'viridis', edgecolor = 'none') plt.title('Loss Surface') plt.xlabel('w') plt.ylabel('b') plt.show() plt.figure() plt.title('Loss Surface Contour') plt.xlabel('w') plt.ylabel('b') plt.contour(self.w, self.b, self.Z) plt.show() # Setter def set_para_loss(self, W, B, loss): self.n = self.n + 1 self.W.append(W) self.B.append(B) self.LOSS.append(loss) # Plot diagram def final_plot(self): ax = plt.axes(projection = '3d') ax.plot_wireframe(self.w, self.b, self.Z) ax.scatter(self.W, self.B, self.LOSS, c = 'r', marker = 'x', s = 200, alpha = 1) plt.figure() plt.contour(self.w, self.b, self.Z) plt.scatter(self.W, self.B, c = 'r', marker = 'x') plt.xlabel('w') plt.ylabel('b') plt.show() # Plot diagram def plot_ps(self): plt.subplot(121) plt.ylim plt.plot(self.x, self.y, 'ro', label = \"training points\") plt.plot(self.x, self.W[-1] * self.x + self.B[-1], label = \"estimated line\") plt.xlabel('x') plt.ylabel('y') plt.ylim((-10, 15)) plt.title('Data Space Iteration: ' + str(self.n)) plt.subplot(122) plt.contour(self.w, self.b, self.Z) plt.scatter(self.W, self.B, c = 'r', marker = 'x') plt.title('Loss Surface Contour Iteration' + str(self.n)) plt.xlabel('w') plt.ylabel('b') plt.show() ``` ## Data setup ```python # Set random seed torch.manual_seed(1) # Setup the actual data and simulated data X = torch.arange(-3, 3, 0.1).view(-1, 1) f = 1 * X - 1 Y = f + 0.1 * torch.randn(X.size()) ``` ## Plotting a simple line ```python # Plot out the data dots and line plt.plot(X.numpy(), Y.numpy(), 'rx', label = 'y') plt.plot(X.numpy(), f.numpy(), label = 'f') plt.xlabel('x') plt.ylabel('y') plt.legend() plt.show ``` ## Create a 3D loss space by giving random weights and biases ```python # Create plot_error_surfaces for viewing the data get_surface = plot_error_surfaces(15, 13, X, Y, 30) ``` ## New plots during each epoch where weights(w) and biases(b) are updated ```python # Section for plotting get_surface.set_para_loss(w.data.tolist(), b.data.tolist(), loss.tolist()) get_surface.plot_ps() ``` ",
    "url": "/Plotting%20Loss/",
    "relUrl": "/Plotting Loss/"
  }
}
