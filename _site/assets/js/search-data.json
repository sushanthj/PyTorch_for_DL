{"0": {
    "doc": "Derivatives in PyTorch",
    "title": "Derivatives in PyTorch",
    "content": "{: .text-delta } 1. TOC {:toc} # Derviates ![](/images/derivates.jpeg) # Partial Derivaties ![](/images/partial_derivatives.jpeg) # Differentiation in Forward/Backward pass of a Neural Net Previously we used 'y.backward' when doing the differentiation Now, let's customize the 'backward' function according the math operator Let's say: **y = 2x** ```python class SQ(torch.autograd.Function): @staticmethod def forward(ctx,i): \"\"\" In the forward pass we receive a Tensor containing the input and return a Tensor containing the output. ctx is a context object that can be used to stash information for backward computation. You can cache arbitrary objects for use in the backward pass using the ctx.save_for_backward method. \"\"\" result=i**2 ctx.save_for_backward(i) return result @staticmethod def backward(ctx, grad_output): \"\"\" In the backward pass we receive a Tensor containing the gradient of the loss with respect to the output, and we need to compute the gradient of the loss with respect to the input. \"\"\" i, = ctx.saved_tensors grad_output = 2*i return grad_output ``` We will now apply the class above using a .apply tag as shown below: ```python x=torch.tensor(2.0,requires_grad=True ) sq=SQ.apply y=sq(x) y print(y.grad_fn) y.backward() x.grad ``` ",
    "url": "/Derivatives/",
    "relUrl": "/Derivatives/"
  },"1": {
    "doc": "Git Concepts",
    "title": "Git Concepts",
    "content": "{: .text-delta } 1. TOC {:toc} # Before you Begin {: .fs-9 } [Reference](https://www.w3schools.com/git/git_getstarted.asp?remote=github){: .btn .fs-5 .mb-4 .mb-md-0} In the above link, follow the procedures, but instead of using username and password each time, setup the ssh keys and use them more often *ssh keys are found in ./.ssh folder (or lookup keygen to generate your keys)* # Basics of generating new content in local and pushing to github ## Process for adding to a github page git add . \\ git commit -m \"made new code\" \\ git push or git push origin develop (if you cloned from develop branch) ## If you want to track a different branch - git branch --set-upstream-to=origin/master \\ git add . \\ git push or make a new remote - git remote add ts_origin_wiki git@github.com:sjayanth21/BR_Wiki.git \\ git push --set-upstream ts_origin_wiki master \\ git push ts_origin_wiki_master ## Working with remotes Any folder can have a number of remotes like: origin and ts_origin_github To make local branch master track a different remote branch (branch in your cloud github repo) do: git branch --set-upstream-to=origin/master or git branch --set-upstream-to=origin/develop ## If you cloned a repo, forked your own branch (using git checkout) You may need to pull from upstream to update your codebase \\ However, running a simple 'git pull' may throw merge conflicts So do the following 1. Run a 'git fetch' to get the updates on all branches (and if any new branch has been added) 2. In your personal branch commit all changes by doing: git add, commit and push 3. sudo apt install meld 4. Now to get the upstream updates do 'git checkout develop' (whichever is the main branch) 5. Now to put this in your personal branch run 'git checkout feature/sj' 6. Now we do the actual merging using 'git merge develop' (this will merge everythin in deveop into the current branch viz feature/sj) 7. The above step would have thrown some merge conflicts, to solve that run 'git mergetool' 8. The above step opens meld, make all necessary resolutions and save 9. Now our codebase would have been updated to whatever we resolved in meld 10. Now run 'git commit' without any arguments as it is a 'merge commit' 11. Now as usual do 'git push origin feature/sj' to push your updated personal branch to github ## Points to Note - If you checkout a file 'git checkout blade.py' it resets the file to whatever is the latest from that branch in upstream - If you want to physically add or change remotes go to the respective folder and do 'nano .git/config' - the correct syntax for the merge command is: \\ 'git merge ts_origin/master' \\ What this does is that if the current branch is origin/develop it will merge the files of \\ current branch i.e origin/develop with ts_origin/master - Note that even if ts_origin/master is in ts_github account and origin/master is in sushanthj github account, it will still merge as long as remotes exist for both these accounts. If remotes don't exist, you can always add as shown up above ### Concepts for working with two repos or two repos on two different github accounts: Basically locally you will have 'master' branch if you do 'git branch' \\ This master can track two upstream branches using two different remotes \\ One remote is added automatically when you clone the repo \\ The next remote will have to be added manually to your other git account or other repo Then to push the same commit to both branches first do 'git push' \\ and see which repo it pushes to (say it pushes to origin/master \\ Then do 'git push --set-upstream ts_origin/develop' to push to your second repo \\ However, do note that your local branch always tracks to the latest branch you pushed to \\ i.e if you do a git pull, it will pull from the latest branch to which you pushed \\ in this case it will pull from ts_origin/develop ### Saving a patch file If you have changes made which you want to save locally and not push to remote, you can save a patch file ```bash git diff > new_changes.patch ``` Now to apply this patch onto any branch, do: ```bash git apply new_changes.patch ``` ### Saving changes by stashing Instead of saving a specific file for changes (such as a patch file), you could also stash your changes locally ``` git stash ``` The above command will stash all tracked changes. You could also stash only committed changes. Refer: [stashing](https://www.atlassian.com/git/tutorials/saving-changes/git-stash) To then apply the stashed changes (one time use only as pop will remove from stash) ``` git stash pop ``` To apply without popping do: ``` git stash apply ``` To remove any particular item in stash: ``` git stash drop ``` To view all entries in stash and then apply specific one do: ``` git stash list git stash apply n ``` n = stash item number ",
    "url": "/git_concepts",
    "relUrl": "/git_concepts"
  },"2": {
    "doc": "Intro",
    "title": "Intro",
    "content": "For Jekyll reference see [just_the_docs](https://pmarsceill.github.io/just-the-docs/) The following pages are built in order to understand Computer Vision and Machine Learning To deploy on heroku follow the steps in the link below (and use the gem files, rake files and proc files in this repo for reference) The following files will need to be copied from this repo: - config.ru - Rakefile - Procfile - static.json - config.yaml (only the differences) And only if necessary: - Gemfile - Gemfile.lock - remove _sites from .gitignore Run bundle exec jekyll serve after making the above changes After copying these files (or their necessary contents), install heroku cli and do: ```bash heroku login ``` Then do heroku create as per the below link and the other steps necessary (git push heroku master) [Deploy jekyll on heroku](https://blog.heroku.com/jekyll-on-heroku) Finally, go to heroku page -> settings -> change the name of the app and find the url ",
    "url": "/intro/",
    "relUrl": "/intro/"
  },"3": {
    "doc": "PyTorch Tensors",
    "title": "PyTorch Tensors",
    "content": "{: .text-delta } 1. TOC {:toc} # Basics In PyTorch all parts of a neural net like weights, datasets, biases, outcomes and FC layers are treated as tensors. One can say that a tensor is a very generalized term for any unit of data in a neural net which is capable of undergoing math operations (like vector ops) ## Creating Tensors All tensors are created in the form CHW (Channel, Height, Width) ```python import torch # three ways of initializing tensors a = torch.tensor([7,4,3,2,6]) b = torch.tensor([0.0, 1.0, 2.0, 3.0], dtype=torch.int32) c = torch.FloatTensor([0,1,2]) ``` ``` >> a[0] = 7 >> a.dtype = torch.int64 >> a.type = LongTensor >> a.size() = torch.tensor(1,5) # you can also try a.ndimension() ``` ## Resizing Tensors Note in the last line of code above we saw that tensor 'a' had 1 row and 5 columns Now to resize we do: ```python a_col = a.view(5,1) # the arguments of view dictate the no. of rows and columns respectively a_col_auto = a.view(-1,1) # -1 infers no. of rows after readjusting the second dim. ``` # Numpy and Tensor relations Preferably don't use numpy and work only with tensors in DL networks as it messes up with ONNX or TRT conversions We will use the function torch.from_numpy and a.numpy() ```python import numpy as np import torch a = np.array([1,2,3,4,5]) torch_tensor = torch.from_numpy(a) numpy_array = torch_tensor.numpy() ordinary_list = torch_tensor.to_list() ``` # Broadcasting Some torch functions can be broadcast along all elements in a tensor or a list ```python import torch import numpy as np a = torch.tensor([0, np.pi/2, 0]) a = torch.sin(a) >> a = [0,1,0] ``` ## Creating an evenly spaced torch tensor We can use the broadcasting feature of a tensor as we described above along with a torch function called linspace Note. linspace is also available in numpy with the function having the same name ```python import torch a = torch.linspace(-2,2,num=5) >> a = [-2, -1, 0, 1, 2] ``` # 2D and 3D Tensors and subscripting In matrices or numpy arrays, accessing individual elements is often called **subscripting** if 'a' is a 2D tensor, then the individual elements can be accesses as: ```python a = [...] >> a[0,2] = # 0th row and 2nd column element ``` # Converting Tensors from one type to another ![](/images/tensor_conversion.jpeg) ",
    "url": "/Tensors/",
    "relUrl": "/Tensors/"
  },"4": {
    "doc": "PyTorch Transforms and Functionals",
    "title": "PyTorch Transforms and Functionals",
    "content": "{: .text-delta } 1. TOC {:toc} ",
    "url": "/transforms_functionals/",
    "relUrl": "/transforms_functionals/"
  },"5": {
    "doc": "Home",
    "title": "Home",
    "content": "# PyTorch for Deep Learning {: .fs-9 } [Courseware](https://www.coursera.org/learn/deep-neural-networks-with-pytorch/home/welcome){: .btn .fs-5 .mb-4 .mb-md-0 } ",
    "url": "/",
    "relUrl": "/"
  }
}
