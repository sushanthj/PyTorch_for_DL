{"0": {
    "doc": "Basics",
    "title": "Basics",
    "content": "{: .text-delta } 1. TOC {:toc} # The following document sumarizes the PyTorch concepts Read this doc after maybe 10% of the Coursera course: https://courses.cs.washington.edu/courses/cse446/19au/section9.html ",
    "url": "/Basics/",
    "relUrl": "/Basics/"
  },"1": {
    "doc": "Derivatives in PyTorch",
    "title": "Derivatives in PyTorch",
    "content": "{: .text-delta } 1. TOC {:toc} # Derviates ![](/images/derivates.jpeg) ## Understanding the variables in derivatives If we have a simple y=x**2 function, the way each tensor gets defined changes. Notice how we define the tensor x: ```python # Create a tensor x x = torch.tensor(2.0, requires_grad = True) print(\"The tensor x: \", x) ``` Now notice how we define y: ```python # Create a tensor y according to y = x^2 y = x ** 2 print(\"The result of y = x^2: \", y) ``` Now that the tensors are defined, there are two steps we need to follow to get the derivative at a specific point: 1. y.backward() 2. x.grad On their own, each would not be sufficient to find grad. However, y.backward() does the actual math part of derivation in the background and the *gradient is stored in the x tensor* Notice the following attributes of x and y and their respective outputs: ![](/images/derivates.jpeg) ## Passing multiple values as input to a function whose gradient we find Here we use the similar linspace function (like numpy) to get a range of values: ```python # Calculate the derivative with multiple values x = torch.linspace(-10, 10, 10, requires_grad = True) Y = x ** 2 y = torch.sum(x ** 2) ``` ## Plotting the above function in matplotlib ```python # Take the derivative with respect to multiple value. Plot out the function and its derivative y.backward() plt.plot(x.detach().numpy(), Y.detach().numpy(), label = 'function') plt.plot(x.detach().numpy(), x.grad.detach().numpy(), label = 'derivative') plt.xlabel('x') plt.legend() plt.show() ``` In the example, we use x.detach and y.detach \\ What this does is ensure that further grads or any values are not added as attributes to x and y tensors Note: The tensors use graphs to populate values of x.grad and y.grad. Hence, these detach functions \\ essentially just disable any further sub-graphs being populated to x or y # Partial Derivaties ![](/images/partial_derivatives.jpeg) # Differentiation in Forward/Backward pass of a Neural Net Previously we used 'y.backward' when doing the differentiation Now, let's customize the 'backward' function according the math operator Let's say: **y = 2x** ```python class SQ(torch.autograd.Function): @staticmethod def forward(ctx,i): \"\"\" In the forward pass we receive a Tensor containing the input and return a Tensor containing the output. ctx is a context object that can be used to stash information for backward computation. You can cache arbitrary objects for use in the backward pass using the ctx.save_for_backward method. \"\"\" result=i**2 ctx.save_for_backward(i) return result @staticmethod def backward(ctx, grad_output): \"\"\" In the backward pass we receive a Tensor containing the gradient of the loss with respect to the output, and we need to compute the gradient of the loss with respect to the input. \"\"\" i, = ctx.saved_tensors grad_output = 2*i return grad_output ``` We will now apply the class above using a .apply tag as shown below: ```python x=torch.tensor(2.0,requires_grad=True ) sq=SQ.apply y=sq(x) y print(y.grad_fn) y.backward() x.grad ``` ## Torch.is_leaf ![](/images/is_leaf.jpeg) ",
    "url": "/Derivatives/",
    "relUrl": "/Derivatives/"
  },"2": {
    "doc": "Git Concepts",
    "title": "Git Concepts",
    "content": "{: .text-delta } 1. TOC {:toc} # Before you Begin {: .fs-9 } [Reference](https://www.w3schools.com/git/git_getstarted.asp?remote=github){: .btn .fs-5 .mb-4 .mb-md-0} In the above link, follow the procedures, but instead of using username and password each time, setup the ssh keys and use them more often *ssh keys are found in ./.ssh folder (or lookup keygen to generate your keys)* # Basics of generating new content in local and pushing to github ## Process for adding to a github page git add . \\ git commit -m \"made new code\" \\ git push or git push origin develop (if you cloned from develop branch) ## If you want to track a different branch - git branch --set-upstream-to=origin/master \\ git add . \\ git push or make a new remote - git remote add ts_origin_wiki git@github.com:sjayanth21/BR_Wiki.git \\ git push --set-upstream ts_origin_wiki master \\ git push ts_origin_wiki_master ## Working with remotes Any folder can have a number of remotes like: origin and ts_origin_github To make local branch master track a different remote branch (branch in your cloud github repo) do: git branch --set-upstream-to=origin/master or git branch --set-upstream-to=origin/develop ## If you cloned a repo, forked your own branch (using git checkout) You may need to pull from upstream to update your codebase \\ However, running a simple 'git pull' may throw merge conflicts So do the following 1. Run a 'git fetch' to get the updates on all branches (and if any new branch has been added) 2. In your personal branch commit all changes by doing: git add, commit and push 3. sudo apt install meld 4. Now to get the upstream updates do 'git checkout develop' (whichever is the main branch) 5. Now to put this in your personal branch run 'git checkout feature/sj' 6. Now we do the actual merging using 'git merge develop' (this will merge everythin in deveop into the current branch viz feature/sj) 7. The above step would have thrown some merge conflicts, to solve that run 'git mergetool' 8. The above step opens meld, make all necessary resolutions and save 9. Now our codebase would have been updated to whatever we resolved in meld 10. Now run 'git commit' without any arguments as it is a 'merge commit' 11. Now as usual do 'git push origin feature/sj' to push your updated personal branch to github ## Points to Note - If you checkout a file 'git checkout blade.py' it resets the file to whatever is the latest from that branch in upstream - If you want to physically add or change remotes go to the respective folder and do 'nano .git/config' - the correct syntax for the merge command is: \\ 'git merge ts_origin/master' \\ What this does is that if the current branch is origin/develop it will merge the files of \\ current branch i.e origin/develop with ts_origin/master - Note that even if ts_origin/master is in ts_github account and origin/master is in sushanthj github account, it will still merge as long as remotes exist for both these accounts. If remotes don't exist, you can always add as shown up above ### Concepts for working with two repos or two repos on two different github accounts: Basically locally you will have 'master' branch if you do 'git branch' \\ This master can track two upstream branches using two different remotes \\ One remote is added automatically when you clone the repo \\ The next remote will have to be added manually to your other git account or other repo Then to push the same commit to both branches first do 'git push' \\ and see which repo it pushes to (say it pushes to origin/master \\ Then do 'git push --set-upstream ts_origin/develop' to push to your second repo \\ However, do note that your local branch always tracks to the latest branch you pushed to \\ i.e if you do a git pull, it will pull from the latest branch to which you pushed \\ in this case it will pull from ts_origin/develop ### Saving a patch file If you have changes made which you want to save locally and not push to remote, you can save a patch file ```bash git diff > new_changes.patch ``` Now to apply this patch onto any branch, do: ```bash git apply new_changes.patch ``` ### Saving changes by stashing Instead of saving a specific file for changes (such as a patch file), you could also stash your changes locally ``` git stash ``` The above command will stash all tracked changes. You could also stash only committed changes. Refer: [stashing](https://www.atlassian.com/git/tutorials/saving-changes/git-stash) To then apply the stashed changes (one time use only as pop will remove from stash) ``` git stash pop ``` To apply without popping do: ``` git stash apply ``` To remove any particular item in stash: ``` git stash drop ``` To view all entries in stash and then apply specific one do: ``` git stash list git stash apply n ``` n = stash item number ",
    "url": "/git_concepts",
    "relUrl": "/git_concepts"
  },"3": {
    "doc": "Intro",
    "title": "Intro",
    "content": "For Jekyll reference see [just_the_docs](https://pmarsceill.github.io/just-the-docs/) The following pages are built in order to understand Computer Vision and Machine Learning To deploy on heroku follow the steps in the link below (and use the gem files, rake files and proc files in this repo for reference) The following files will need to be copied from this repo: - config.ru - Rakefile - Procfile - static.json - config.yaml (only the differences) And only if necessary: - Gemfile - Gemfile.lock - remove _sites from .gitignore Run bundle exec jekyll serve after making the above changes After copying these files (or their necessary contents), install heroku cli and do: ```bash heroku login ``` Then do heroku create as per the below link and the other steps necessary (git push heroku master) [Deploy jekyll on heroku](https://blog.heroku.com/jekyll-on-heroku) Finally, go to heroku page -> settings -> change the name of the app and find the url ",
    "url": "/intro/",
    "relUrl": "/intro/"
  },"4": {
    "doc": "PyTorch Tensors",
    "title": "PyTorch Tensors",
    "content": "{: .text-delta } 1. TOC {:toc} # Basics In PyTorch all parts of a neural net like weights, datasets, biases, outcomes and FC layers are treated as tensors. One can say that a tensor is a very generalized term for any unit of data in a neural net which is capable of undergoing math operations (like vector ops) ## Creating Tensors All tensors are created in the form CHW (Channel, Height, Width) ```python import torch # three ways of initializing tensors a = torch.tensor([7,4,3,2,6]) b = torch.tensor([0.0, 1.0, 2.0, 3.0], dtype=torch.int32) c = torch.FloatTensor([0,1,2]) ``` ``` >> a[0] = 7 >> a.dtype = torch.int64 >> a.type = LongTensor >> a.size() = torch.tensor(1,5) # you can also try a.ndimension() ``` ## Resizing Tensors Note in the last line of code above we saw that tensor 'a' had 1 row and 5 columns Now to resize we do: ```python a_col = a.view(5,1) # the arguments of view dictate the no. of rows and columns respectively a_col_auto = a.view(-1,1) # -1 infers no. of rows after readjusting the second dim. ``` # Numpy and Tensor relations Preferably don't use numpy and work only with tensors in DL networks as it messes up with ONNX or TRT conversions We will use the function torch.from_numpy and a.numpy() ```python import numpy as np import torch a = np.array([1,2,3,4,5]) torch_tensor = torch.from_numpy(a) numpy_array = torch_tensor.numpy() ordinary_list = torch_tensor.to_list() ``` # Broadcasting Some torch functions can be broadcast along all elements in a tensor or a list ```python import torch import numpy as np a = torch.tensor([0, np.pi/2, 0]) a = torch.sin(a) >> a = [0,1,0] ``` ## Creating an evenly spaced torch tensor We can use the broadcasting feature of a tensor as we described above along with a torch function called linspace Note. linspace is also available in numpy with the function having the same name ```python import torch a = torch.linspace(-2,2,num=5) >> a = [-2, -1, 0, 1, 2] ``` # 2D and 3D Tensors and subscripting In matrices or numpy arrays, accessing individual elements is often called **subscripting** if 'a' is a 2D tensor, then the individual elements can be accesses as: ```python a = [...] >> a[0,2] = # 0th row and 2nd column element ``` # Converting Tensors from one type to another ![](/images/tensor_conversion.jpeg) ",
    "url": "/Tensors/",
    "relUrl": "/Tensors/"
  },"5": {
    "doc": "PyTorch Transforms and Functionals",
    "title": "PyTorch Transforms and Functionals",
    "content": "{: .text-delta } 1. TOC {:toc} All transforms come under the module **torchvision.transforms** and can be used within the layer of a neural net we usually import this as: ```python import torchvision.transforms as T ``` # Basic operations without transforms ## Changing Image Channels (aka flipping elements in any specified dimension) The way an image is loaded varies with implementations: - pytorch: CHW (channels, width, height) - opencv: HWC (height, width, channels) - numpy: WHC (width, height, channels) *this is also what imagej shows* FYI, **loading an image in opencv loads it as BGR channels** \\ To correct this we usually do cv2.cvt_Color(image, cv2.BGR2RGB) However, if we need to do the same in numpy or on torch, we essentially just need to flip the dimensions of channels B and R (or we just say flip dimension no. 0 of image tensor) ```python import torch flipped_image = torch.flip(input_tensor,[0]) # as CHW, C is in dimension [0] ``` # Tensor operations using torchvision.transform *Note: To understand better about image loading, lookup the 'Dataset Examples/image_dataset_from_scratch'* ## Padding There are two ways of padding: ### Padding using a mask tensor Lets say we have an image of size (CHW) = (3, 1200, 1300) \\ Now let's assume we need to convert it into a square image *Note. I've used padding on a rectangular image mostly in order to maintain aspect ratio during resize operations* #### Create a zero mask ```python import torch pad_mask = torch.as_tensor(torch.zeros(3,1300,1300, dtype=torch.int8)) ``` #### Copy the original image onto the mask Now the original image was of size: - orig image: (3,1200,1300) - pad_mask: (3,1300,1300) Therefore, we see that we need to only pad 50 pixels above and below in orig image ```python pad_mask[:,50:1250,:] = orig_image ``` pad_mask will be our final padded image now ### Padding using a torch transform ```python import torch import torchvision.transforms as T # we pass a tuple to the pad function below where (x,y) # x = pad amount in left and right # y = pad amount in top and bottom padder = T.Pad((0,50)) padded_img = padder(input_image) ``` ## Resizing ```python import torchvision.transforms as T # define the interpolation method # we're using NEAREST as default BILINEAR is not supported during ONNX conversion interpolation = T.InterpolationMode.NEAREST resizer = T.Resize((640,640), interpolation=interpolation) resized_image = resizer(input_image) ``` ## Multiple transforms at once (Composing): ```python from torchvision import transforms data_transform = transforms.Compose([transformer_1(), transformer_2()]) x, y = data_transform(x1, y1) ``` # Torch Functionals Many common tensor operations (used in DNNs) and few other meta operations are bundled in a class called **Module** Let's create a custom layer by subclassing the Module class (standard procedure) ```python import torch.nn as nn class Custom_Layer(nn.Module): def __init__(self): super().__init__() self.linear = nn.Linear(2,3) def forward(self,input): x = self.linear(input) return x ``` Here we saw that **nn.Linear** was used, but there are also other functions like \\ nn.Conv2D and nn.ReLu which are commonly used These functions like nn.ReLu are said to be of the form **torch.nn.functional** i.e. Relu and Conv2D are all **torch functionals** ## Using torchvision.transformations inside functionals Let's club together resizing and padding and put them in a functional called **Sequential** As the name suggests, Sequential executes anhy operations one after another ```python import torchvision.transforms as T import torch interpolation = T.InterpolationMode.NEAREST transformer = torch.nn.Sequential(T.Pad((0,pad_const)),T.Resize((640,640),interpolation=interpolation)) output_image = transformer(input_image) ``` ## Creating Meta Models Let's say we have an existing model in pytorch called *pretrained_model* \\ and we want to add additional layers to it in the beginning or the end Usually a model has a few class variables as well like stride, no. of classes etc. We will have to retain this info in our meta model as well Now let's define our custom model: ```python import torch import torchvision.transforms as T class Custom_Layer(nn.Module): # Custom layer for preprocessing def __init__(self): super().__init__() def forward(self,input_tensor): input_tensor = input_tensor.type(torch.cuda.FloatTensor) #print(\"input tensor type: \", input_tensor.dtype) pad_mask = torch.as_tensor(torch.zeros(3, 1328, 1328, dtype=torch.float32), device=0) flipped_image = torch.flip(input_tensor,[1]) pad_mask[:,64:1264,:] = flipped_image interpolation = T.InterpolationMode.NEAREST #transformer = torch.nn.Sequential(T.Pad((0,pad_const)),T.Resize((640,640),interpolation=interpolation)) transformer = torch.nn.Sequential(T.Resize((640,640),interpolation=interpolation)) transformed_tensor = transformer(pad_mask).unsqueeze(0) return transformed_tensor class Custom_Model(nn.Module): def __init__(self, pretrained_model, nc, names, stride): super(Custom_Model, self).__init__() self.nc = nc self.names = names self.preproc_layers = Custom_Layer() self.pretrained = pretrained_model self.stride = stride def forward(self, input): # add with no grad condition for the next line? mod = self.preproc_layers(input) mod = self.pretrained(mod) return mod ``` We have defined in the above code a custom layer and a custom model. You may have observed that we used torch.cuda.float16. This is because the existing tensors of the pretrained_model are all loaded onto GPU. To interface with them, we need to initialize our new tensors on the GPU as well. Now, let's call this custom model in a seperate function: ```python def custom_load(weights_file_path, device): existing_model, stride = attempt_load(weights=weights_file_path, map_location=device, inplace=True, fuse=True) nc_existing, names_existing = existing_model.nc, existing_model.names extended_model = Custom_Model(pretrained_model=existing_model, nc=nc_existing, names=names_existing, stride=stride) return extended_model ``` ",
    "url": "/transforms_functionals/",
    "relUrl": "/transforms_functionals/"
  },"6": {
    "doc": "Datasets",
    "title": "Datasets",
    "content": "{: .text-delta } 1. TOC {:toc} # Dataset examples To create our own dataset, we need to subclass the 'Dataset' class from torch.utils.data **The torch.utils.data also has an in-built DataLoader** Main Points to note: - Each dataset class we define customly, should have the functions **__init__**, **__len__**, and **__getitem__** - Each transformer class we define must have **__init__** and **__call__** functions ## Simple Dataset Refer this link to understand basic datasets (and torchvision.transforms on this dataset): [simple_dataset](/ref_code/simple_dataset.ipynb) ## Image Dataset 1. [image_dataset_from_scratch](/ref_code/image_dset_from_scratch.ipynb) 2. [image_dataset_pre_built](/ref_code/image_dset_built.ipynb) ## Sample Datasets Torchvision has some freely avialable datasets which one can use for a cursory evaluation of a model ```python import torchvision.datasets as dsets dataset = dsets.MNIST(root='./data/', train=False, download=True, transform=transforms.ToTensor()) ``` *Note: in the above dataset, if we say train=False, then we are using the test dataset (smaller set)* *Side Note: Usually train, dev and test are split in 80,10,10 ratio from the total no. of images* ",
    "url": "/Datasets/",
    "relUrl": "/Datasets/"
  },"7": {
    "doc": "Home",
    "title": "Home",
    "content": "# PyTorch for Deep Learning {: .fs-9 } [Courseware](https://www.coursera.org/learn/deep-neural-networks-with-pytorch/home/welcome){: .btn .fs-5 .mb-4 .mb-md-0 } ",
    "url": "/",
    "relUrl": "/"
  }
}
